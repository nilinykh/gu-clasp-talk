
@InProceedings{xu2015,
  title = 	 {Show, Attend and Tell: Neural Image Caption Generation with Visual Attention},
  author = 	 {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {2048--2057},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/xuc15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/xuc15.html},
  abstract = 	 {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.}
}

@inproceedings{Gurari2020CaptioningIT,
  title={Captioning Images Taken by People Who Are Blind},
  author={Danna Gurari and Yinan Zhao and Meng Zhang and Nilavra Bhattacharya},
  booktitle={ECCV},
  year={2020}
}

@inproceedings{papineni2002,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P02-1040",
    doi = "10.3115/1073083.1073135",
    pages = "311--318",
}

@misc{mao2014explain,
      title={Explain Images with Multimodal Recurrent Neural Networks}, 
      author={Junhua Mao and Wei Xu and Yi Yang and Jiang Wang and Alan L. Yuille},
      year={2014},
      eprint={1410.1090},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{karpathy2015deep,
      title={Deep Visual-Semantic Alignments for Generating Image Descriptions}, 
      author={Andrej Karpathy and Li Fei-Fei},
      year={2015},
      eprint={1412.2306},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{vinyals2015tell,
      title={Show and Tell: A Neural Image Caption Generator}, 
      author={Oriol Vinyals and Alexander Toshev and Samy Bengio and Dumitru Erhan},
      year={2015},
      eprint={1411.4555},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@misc{donahue2016longterm,
      title={Long-term Recurrent Convolutional Networks for Visual Recognition and Description}, 
      author={Jeff Donahue and Lisa Anne Hendricks and Marcus Rohrbach and Subhashini Venugopalan and Sergio Guadarrama and Kate Saenko and Trevor Darrell},
      year={2016},
      eprint={1411.4389},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@misc{chen2014learning,
      title={Learning a Recurrent Visual Representation for Image Caption Generation}, 
      author={Xinlei Chen and C. Lawrence Zitnick},
      year={2014},
      eprint={1411.5654},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{xu2016show,
      title={Show, Attend and Tell: Neural Image Caption Generation with Visual Attention}, 
      author={Kelvin Xu and Jimmy Ba and Ryan Kiros and Kyunghyun Cho and Aaron Courville and Ruslan Salakhutdinov and Richard Zemel and Yoshua Bengio},
      year={2016},
      eprint={1502.03044},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{bahdanau2016neural,
      title={Neural Machine Translation by Jointly Learning to Align and Translate}, 
      author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
      year={2016},
      eprint={1409.0473},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{ghanimifard2019,
    title = "What goes into a word: generating image descriptions with top-down spatial knowledge",
    author = "Ghanimifard, Mehdi  and
      Dobnik, Simon",
    booktitle = "Proceedings of the 12th International Conference on Natural Language Generation",
    month = oct # "{--}" # nov,
    year = "2019",
    address = "Tokyo, Japan",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-8668",
    doi = "10.18653/v1/W19-8668",
    pages = "540--551",
    abstract = "Generating grounded image descriptions requires associating linguistic units with their corresponding visual clues. A common method is to train a decoder language model with attention mechanism over convolutional visual features. Attention weights align the stratified visual features arranged by their location with tokens, most commonly words, in the target description. However, words such as spatial relations (e.g. \textit{next to} and \textit{under}) are not directly referring to geometric arrangements of pixels but to complex geometric and conceptual representations. The aim of this paper is to evaluate what representations facilitate generating image descriptions with spatial relations and lead to better grounded language generation. In particular, we investigate the contribution of three different representational modalities in generating relational referring expressions: (i) pre-trained convolutional visual features, (ii) different top-down geometric relational knowledge between objects, and (iii) world knowledge captured by contextual embeddings in language models.",
}

@INPROCEEDINGS{lu2017,  author={Lu, Jiasen and Xiong, Caiming and Parikh, Devi and Socher, Richard},  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},   title={Knowing When to Look: Adaptive Attention via a Visual Sentinel for Image Captioning},   year={2017},  volume={},  number={},  pages={3242-3250},  doi={10.1109/CVPR.2017.345}}

@InProceedings{ilinykh2021,
  author    = {Ilinykh, Nikolai  and  Dobnik, Simon},
  title     = {How Vision Affects Language: Comparing Masked Self-Attention in Uni-Modal and Multi-Modal Transformer},
  booktitle      = {Proceedings of the 1st Workshop on Multimodal Semantic Representations (MMSR)},
  month          = {June},
  year           = {2021},
  address        = {Groningen, Netherlands (Online)},
  publisher      = {Association for Computational Linguistics},
  pages     = {45--55},
  abstract  = {The problem of interpretation of knowledge learned by multi-head self-attention in transformers has been one of the central questions in NLP. However, a lot of work mainly focused on models trained for uni-modal tasks, e.g. machine translation. In this paper, we examine masked self-attention in a multi-modal transformer trained for the task of image captioning. In particular, we test whether the multi-modality of the task objective affects the learned attention patterns. Our visualisations of masked self-attention demonstrate that (i) it can learn general linguistic knowledge of the textual input, and (ii) its attention patterns incorporate artefacts from visual modality even though it has never accessed it directly. We compare our transformer's attention patterns with masked attention in distilgpt-2 tested for uni-modal text generation of image captions. Based on the maps of extracted attention weights, we argue that masked self-attention in image captioning transformer seems to be enhanced with semantic knowledge from images, exemplifying joint language-and-vision information in its attention patterns.},
  url       = {https://www.aclweb.org/anthology/2021.mmsr-1.5}
}


